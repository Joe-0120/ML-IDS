{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af42d4d-2d81-47ca-9b9d-ca3ddc2feb99",
   "metadata": {},
   "source": [
    "# Optimizing hyperparameters for DL models on the IDS2017 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fb2b1-392c-46a4-9f4d-ee27dd1cafa8",
   "metadata": {},
   "source": [
    "In this notebook, different DL models are used on the IDS2017 with hyperparameter optimization to test the performance. Deep neural networks, autoencoders, convolutional networks and RNNs are tested on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70920519-5002-4730-8b20-0d38fcdabbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ids2018 import load_ids2018, feature_selection\n",
    "from notebook_utils import plot_confusion_matrix, metrics_report, calculate_metrics_by_label, test_metrics_DL, plot_overall_accuracy\n",
    "from notebook_utils import test_metrics_AE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "attack_labels = {\n",
    "    0: 'Benign',\n",
    "    1: 'Bot',\n",
    "    2: 'Brute Force -Web',\n",
    "    3: 'Brute Force -XSS',\n",
    "    4: 'DDOS attack-HOIC',\n",
    "    5: 'DDOS attack-LOIC-UDP',\n",
    "    6: 'DDoS attacks-LOIC-HTTP',\n",
    "    7: 'DoS attacks-GoldenEye',\n",
    "    8: 'DoS attacks-Hulk',\n",
    "    9: 'DoS attacks-SlowHTTPTest',\n",
    "    10: 'DoS attacks-Slowloris',\n",
    "    11: 'FTP-BruteForce',\n",
    "    12: 'Infilteration',\n",
    "    13: 'SQL Injection',\n",
    "    14: 'SSH-Bruteforce'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a88b7-ec76-4d21-ac86-45010923ae63",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308bff8-b683-4875-af61-9a4c13809a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_ids2018()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7431eee-cf60-4067-8340-c6f451ce55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:78]\n",
    "Y = df[[\"label\", \"is_attack\", \"label_code\"]]\n",
    "\n",
    "X.info()\n",
    "Y.info()\n",
    "print(Y.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c8af7-6ee8-45f3-b116-75284974903d",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ef6e9-83bc-495b-841d-27c1830bebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_selection(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4927221-6c5b-4bf5-bad6-805d86ceaded",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbb370-f269-4694-8b8c-87dd9e3f927c",
   "metadata": {},
   "source": [
    "The dataset is split into a training set and a testing set with a ratio of 0.8/0.2. The dataset is stratified according to the label to have an equal representation of all classes in the 2 subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658a1ce-7544-47cd-9fbe-cab2b6445033",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y.label_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23761a9a-1cb5-4ec7-a357-c89dfb5a6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6e09b-ddde-4d4b-9239-f391b4256452",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c6bd8-98fc-4436-9c84-f51aa5684003",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_percentage = len(Y_train.label[Y_train[\"label\"]==\"BENIGN\"])/len(Y_train)\n",
    "print('Percentage of benign samples: %.4f' % benign_percentage)\n",
    "print(Y_train.is_attack.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274ff54-940a-445c-ad52-9a38259f93cd",
   "metadata": {},
   "source": [
    "## Smote Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f93cd-f690-436b-a930-5acbb01dd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def resample_dataset(X, Y, min_samples, attack_labels):\n",
    "    Y = Y.drop(columns=['label'])\n",
    "    combined = pd.concat([X, Y], axis=1)\n",
    "    counts = Y['label_code'].value_counts()\n",
    "    samples_number = {i: max(counts[i], min_samples) for i in np.unique(Y['label_code'])}\n",
    "    combined_array = combined.values\n",
    "    y_array = Y['label_code'].values\n",
    "    resampler = SMOTE(random_state=42, sampling_strategy=samples_number)\n",
    "    resampled_array, y_resampled = resampler.fit_resample(combined_array, y_array)\n",
    "    X_resampled = resampled_array[:, :-Y.shape[1]]\n",
    "    Y_resampled = resampled_array[:, -Y.shape[1]:]\n",
    "    X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    Y_resampled_df = pd.DataFrame(Y_resampled, columns=Y.columns)\n",
    "    Y_resampled_df['label'] = Y_resampled_df['label_code'].map(attack_labels)\n",
    "    Y_resampled_df['label'] = Y_resampled_df['label'].astype('category')\n",
    "    return X_resampled_df, Y_resampled_df\n",
    "\n",
    "X_smote_train, Y_smote_train = resample_dataset(X_train, Y_train, 100000, attack_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0854798-4fb5-4ee7-b640-4df5a46bbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_smote_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c8ada-8fbf-42da-912c-64462d3a4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_smote = StandardScaler()\n",
    "scaler_smote.fit(X_smote_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ec577-bd12-47c0-83a3-392b45457475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "def save_model(model, model_name):\n",
    "    # Create directory if it does not exist\n",
    "    model_dir = os.path.join(\"models\", \"DL_models_optimized_2018\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    # Save the model\n",
    "    model.save(os.path.join(model_dir, f\"{model_name}.keras\"))\n",
    "\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d58edd-aae3-4d95-a0d5-635677892bdd",
   "metadata": {},
   "source": [
    "## Optimized DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce14d4b-ef38-42c7-acee-65987bc2e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the combined model builder function\n",
    "def build_combined_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_smote_train.shape[1],)))\n",
    "\n",
    "    # Choose the number of layers (either 3 or 4)\n",
    "    num_layers = hp.Choice('num_layers', values=[3, 4])\n",
    "    \n",
    "    # Same dropout rate for all layers\n",
    "    dropout_rate = hp.Choice('dropout', values=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "    if num_layers == 3:\n",
    "        # Fixed units for 3 layers: 128, 64, 32\n",
    "        units_per_layer = [128, 64, 32]\n",
    "    else:\n",
    "        # Fixed units for 4 layers: 256, 128, 64, 32\n",
    "        units_per_layer = [256, 128, 64, 32]\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(units=units_per_layer[i], activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Use Adam optimizer with different learning rates\n",
    "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner for the combined model\n",
    "tuner_combined = kt.Hyperband(build_combined_model,\n",
    "                              objective='accuracy',\n",
    "                              max_epochs=20,\n",
    "                              factor=3,\n",
    "                              directory='optimization_2018',\n",
    "                              project_name='DNN_combined_fixed_dropout')\n",
    "\n",
    "# Early stopping callback\n",
    "stop_early = EarlyStopping(monitor='accuracy', patience=5)\n",
    "\n",
    "# Perform hyperparameter search for the combined model\n",
    "tuner_combined.search(scaler_smote.transform(X_smote_train), Y_smote_train.is_attack, \n",
    "                      epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps_combined = tuner_combined.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"The optimal hyperparameters are: {best_hps_combined.values}\")\n",
    "\n",
    "# Build and train the model with the optimal hyperparameters\n",
    "model_combined = tuner_combined.hypermodel.build(best_hps_combined)\n",
    "history_combined = model_combined.fit(scaler_smote.transform(X_smote_train), Y_smote_train.is_attack, \n",
    "                                      epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Evaluate and save the model\n",
    "metrics[\"DNN_Optimized_Combined\"] = test_metrics_DL(\"DNN_Optimized_Combined\", model_combined, scaler_smote, X_test, Y_test, reshape=False)\n",
    "save_model(model_combined, \"DNN_SMOTE_Optimized_Combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7ae98-8dc1-4abe-92a1-20c5457f5286",
   "metadata": {},
   "source": [
    "## Optimized CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfe466-fd59-4724-9604-5609a66bc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Flatten, Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensure TensorFlow compatibility\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model builder function for CNN with fixed units and filters, and reduced dropout range\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_smote_train.shape[1], 1)))\n",
    "    \n",
    "    # Fixed number of filters in the Conv1D layer\n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=hp.Int('kernel_size', min_value=2, max_value=5, step=1),\n",
    "                     activation='relu'))\n",
    "    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layer with fixed units\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    \n",
    "    # Dropout layer with reduced range\n",
    "    model.add(Dropout(rate=hp.Choice('dropout', values=[0.0, 0.2, 0.4])))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Adam optimizer with different learning rates\n",
    "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(build_cnn_model,\n",
    "                     objective='accuracy',\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     directory='optimization_2018',\n",
    "                     project_name='cnn_tuning_fixed_units_filters')\n",
    "\n",
    "# Early stopping callback\n",
    "stop_early = EarlyStopping(monitor='accuracy', patience=5)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(scaler_smote.transform(X_smote_train).reshape(-1, X_smote_train.shape[1], 1), \n",
    "             Y_smote_train.is_attack, \n",
    "             epochs=50, \n",
    "             validation_split=0.2, \n",
    "             callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"The optimal hyperparameters are: {best_hps.values}\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(scaler_smote.transform(X_smote_train).reshape(-1, X_smote_train.shape[1], 1), \n",
    "                    Y_smote_train.is_attack, \n",
    "                    epochs=50, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[stop_early])\n",
    "\n",
    "# Evaluate and save the model\n",
    "metrics[\"CNN_Optimized\"] = test_metrics_DL(\"CNN_Optimized\", model, scaler_smote, X_test, Y_test, reshape=False)\n",
    "save_model(model, \"CNN_SMOTE_Optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65f5e1-eb51-48c3-a9b7-91ea23e2a17f",
   "metadata": {},
   "source": [
    "## RNN Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c4ae9-da15-4b4a-adcf-148fc9aa8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the updated balanced model builder function for RNN\n",
    "def build_updated_rnn_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_smote_train.shape[1], 1)))\n",
    "\n",
    "    # Tune the number of units in the LSTM layer\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=64, max_value=128, step=32)))\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=64, step=32), activation='relu'))\n",
    "\n",
    "    # Dropout layer with a range from 0 to 0.5\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.4, step=0.2)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Adam optimizer with the original set of learning rates\n",
    "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(build_updated_rnn_model,\n",
    "                     objective='accuracy',\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     directory='optimization_2018',\n",
    "                     project_name='updated_rnn_tuning')\n",
    "\n",
    "# Early stopping callback\n",
    "stop_early = EarlyStopping(monitor='accuracy', patience=5)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(scaler_smote.transform(X_smote_train).reshape(-1, X_smote_train.shape[1], 1), \n",
    "             Y_smote_train.is_attack, \n",
    "             epochs=50, \n",
    "             validation_split=0.2, \n",
    "             callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"The optimal hyperparameters are: {best_hps.values}\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(scaler_smote.transform(X_smote_train).reshape(-1, X_smote_train.shape[1], 1), \n",
    "                    Y_smote_train.is_attack, \n",
    "                    epochs=50, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[stop_early])\n",
    "\n",
    "# Evaluate and save the model\n",
    "metrics[\"RNN_Optimized\"] = test_metrics_DL(\"RNN_Optimized\", model, scaler_smote, X_test, Y_test, reshape=False)\n",
    "save_model(model, \"RNN_SMOTE_Optimized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64259ddd-d3c5-416f-9fc9-2e036b2ee0c3",
   "metadata": {},
   "source": [
    "## Optimized Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c887cae-b3ec-424e-9280-f20b98e188f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Filter the training data to include only benign samples\n",
    "benign_data = X_train[Y_train['is_attack'] == 0]\n",
    "# Standardize the benign data\n",
    "scaler_AE = StandardScaler()\n",
    "scaler_AE.fit(benign_data)\n",
    "\n",
    "# Define the model builder function for Autoencoder\n",
    "def build_autoencoder_model_with_threshold(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(benign_data.shape[1],)))\n",
    "\n",
    "    # Encoder\n",
    "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('bottleneck', min_value=8, max_value=16, step=8), activation='relu'))\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    \n",
    "    # Reconstruct the input\n",
    "    model.add(Dense(benign_data.shape[1], activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(build_autoencoder_model_with_threshold,\n",
    "                     objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     directory='optimization_2018',\n",
    "                     project_name='autoencoder_tuning_with_threshold')\n",
    "\n",
    "# Early stopping callback\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Perform hyperparameter search, including threshold tuning\n",
    "tuner.search(scaler_AE.transform(benign_data), scaler_AE.transform(benign_data), \n",
    "             epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"The optimal hyperparameters are: {best_hps.values}\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(scaler_AE.transform(benign_data), scaler_AE.transform(benign_data), \n",
    "                    epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3a2fa-b95b-41aa-a7bb-3010db856554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction errors on the validation set\n",
    "reconstructions = model.predict(scaler_AE.transform(benign_data))\n",
    "reconstruction_errors = np.mean(np.square(scaler_AE.transform(benign_data) - reconstructions), axis=1)\n",
    "\n",
    "# Tune threshold by trying different percentiles\n",
    "best_threshold = None\n",
    "best_accuracy = -1\n",
    "percentiles = range(80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    metrics[\"AE_Optimized\"], metrics_by_label = test_metrics_AE(\n",
    "        \"AE_Optimized\", model, scaler_AE, X_test, Y_test, threshold=threshold)\n",
    "\n",
    "    # Assuming you have some F1 score metric in your `metrics` dictionary\n",
    "    if metrics[\"AE_Optimized\"][\"accuracy\"] > best_accuracy:\n",
    "        best_accuracy = metrics[\"AE_Optimized\"][\"accuracy\"]\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold}, Best F1 Score: {best_f1_score}\")\n",
    "\n",
    "# Save the model\n",
    "save_model(model, \"AE_SMOTE_Optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab8dcb-42d8-45d6-a596-169792e7ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Autoencoder model\n",
    "def create_autoencoder_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(input_shape, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946a123-2d2e-4818-8b6a-3e48bd4ba079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_autoencoder_model_with_fixed_layers(hp):\n",
    "    model = AnomalyDetector(input_shape=benign_data.shape[1])\n",
    "    \n",
    "    # Compile the model with a tunable learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')),\n",
    "                  loss='mae')\n",
    "    \n",
    "    return model\n",
    "# Instantiate the tuner for learning rate search\n",
    "tuner = kt.RandomSearch(\n",
    "    build_autoencoder_model_with_fixed_layers,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='optimization_2018',\n",
    "    project_name='autoencoder2'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(scaler_AE.transform(benign_data), scaler_AE.transform(benign_data),\n",
    "             epochs=50, batch_size=512, validation_split=0.2,\n",
    "             callbacks=[EarlyStopping(monitor='loss', patience=5)])\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Recalculate the threshold using the best model\n",
    "reconstructions = best_model.predict(scaler_AE.transform(benign_data))\n",
    "train_loss = np.mean(np.abs(scaler_AE.transform(benign_data) - reconstructions), axis=1)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Optimal Learning Rate Threshold: \", threshold)\n",
    "\n",
    "# Calculate the loss on validation data (you can use a part of benign_data for validation)\n",
    "validation_loss = np.mean(np.abs(scaler_AE.transform(X_val) - best_model.predict(scaler_AE.transform(X_val))), axis=1)\n",
    "\n",
    "# Test a range of thresholds\n",
    "best_threshold = None\n",
    "best_f1_score = 0\n",
    "for t in np.linspace(np.min(validation_loss), np.max(validation_loss), 100):\n",
    "    Y_pred = (validation_loss > t).astype(int)\n",
    "    f1 = f1_score(Y_val['is_attack'], Y_pred)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold}, Best F1 Score: {best_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8c646-ddf8-4c8f-93c7-030710d692d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"AE\"] = test_metrics_AE(\"Tuned AE\", best_model, scaler_AE, X_test, Y_test, best_threshold)\n",
    "save_model(best_model, \"Tuned_AE_SMOTE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
